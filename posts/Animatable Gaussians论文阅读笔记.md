---
title: Animatable Gaussians 论文阅读笔记
date: 2025-11-10
excerpt: CVPR 2024论文，提出将3D Gaussian Splatting与2D CNN结合的虚拟化身建模方法，实现高保真动态细节和实时渲染。
tags: ['论文', '3DGS', 'Avatar', 'CVPR']
readTime: 30分钟
---

# Animatable Gaussians: Learning Pose-dependent Gaussian Maps for High-fidelity Human Avatar Modeling

## 论文基本信息

**标题**: Animatable Gaussians: Learning Pose-dependent Gaussian Maps for High-fidelity Human Avatar Modeling

**作者**: 
- Zhe Li* (清华大学)
- Yipengjing Sun* (哈尔滨工业大学威海)
- Zerong Zheng (NNKosmos Technology)
- Lizhen Wang (清华大学)
- Shengping Zhang (哈尔滨工业大学威海)
- Yebin Liu (清华大学, IEEE Member)

*共同第一作者

**发表**: CVPR 2024

**论文链接**: 
- arXiv: https://arxiv.org/abs/2311.16096v4
- 项目主页: https://animatable-gaussians.github.io/relight

**代码仓库**: https://github.com/lizhe00/AnimatableGaussians (⭐ 700+)

**扩展版本**: 本文是初步版本的扩展，增加了基于物理的渲染(PBR)以支持重光照功能

---

## 一、问题背景与核心挑战

### 要解决的核心问题

从RGB视频建模**可动画和可重光照的高保真人体虚拟化身**，主要挑战包括：

1. **MLP低频偏差问题**
   - 基于NeRF的方法使用纯MLP难以回归姿态依赖的服装细节
   - MLP存在低频谱偏差，导致平滑或模糊的质量
   - 渲染速度慢（需要沿射线密集采样）

2. **宽松衣物建模困难**
   - 现有方法难以建模远离人体的宽松服装（如长裙）
   - 需要适应穿着服装的参数化模板

3. **新姿态泛化能力不足**
   - 直接外推到分布外姿态会产生不满意结果
   - 需要合理插值策略

4. **缺乏重光照能力**
   - 现有可动画虚拟化身只能在捕获环境光照下动画
   - 需要分解材质和照明以支持新照明下的渲染

### 研究动机

- **显式点云表示的潜力**：3D Gaussian Splatting提供高保真渲染质量和实时速度
- **2D CNN的强大能力**：显式表示可以在2D图上参数化，使用更强大的2D CNN
- **物理真实感需求**：需要支持在不同照明条件下的真实动画

---

## 二、核心技术方法

### 方法概览

Animatable Gaussians提出了一种**新的虚拟化身表示**，结合：
- ✅ 3D Gaussian Splatting（显式基于点的表示）
- ✅ 强大的2D CNN（StyleGAN-based StyleUNet）
- ✅ 模板引导的参数化
- ✅ PCA姿态投影策略
- ✅ 基于物理的渲染(PBR)

### 算法流程图

```
                            输入
                             ↓
        ┌────────────────────┼────────────────────┐
        │                                          │
    多视图RGB视频                            SMPL-X参数
    (1500-3000帧)                        (姿态、形状、表情等)
        │                                          │
        └────────────────────┬────────────────────┘
                             ↓
                    ┌────────────────┐
                    │ 步骤1: 模板重建 │
                    └────────────────┘
                             ↓
              SDF & Color Field (MLP)
                             ↓
                  规范空间几何模板 + 蒙皮权重
                             ↓
                    ┌────────────────────┐
                    │ 步骤2: 姿态驱动网络 │
                    └────────────────────┘
                             ↓
        ┌────────────────────┼────────────────────┐
        │                    │                    │
    LBS变形             渲染位置图          提取姿态条件
        │              Pf(Θ) + Pb(Θ)              │
        │               512×512                   │
        │                    │                    │
        │           ┌────────┴────────┐           │
        │           │  StyleUNet (×5) │           │
        │           └────────┬────────┘           │
        │                    ↓                    │
        │         正面+背面输出 1024×1024         │
        │    ┌────────────────────────────┐       │
        │    │ • 颜色图 (3ch)             │       │
        │    │ • 位置偏移图 (3ch)         │       │
        │    │ • 其他属性 (8ch)           │       │
        │    │ • 反照率+粗糙度 (4ch)      │       │
        │    │ • 法线+可见性 (19ch)       │       │
        │    └────────────────────────────┘       │
        │                    │                     │
        │         提取规范3D高斯                   │
        │                    │                     │
        └────────────────────┼─────────────────────┘
                             ↓
                    LBS变形到姿态空间
                             ↓
                ┌────────────┴────────────┐
                │                         │
        观察颜色分支               PBR颜色分支
                │                         │
                ↓                         ↓
        3DGS点绘渲染              PBR + 点绘渲染
                │                         │
                └────────────┬────────────┘
                             ↓
                      渲染图像输出
```

### 1. 模板引导的参数化

**学习参数化模板**：
```
选择A姿态帧 → 优化SDF/颜色场 → Marching Cubes提取模板 → 扩散蒙皮权重
```

**2D参数化**：
- 通过正交投影将3D模板投影到正面和背面视图
- 分辨率：512×512 (输入) → 1024×1024 (输出)
- 每个像素代表一个3D高斯

### 2. 姿态依赖的高斯图

使用**StyleUNet**从姿态位置图预测：
- 颜色 (3ch)
- 位置偏移 (3ch)
- 不透明度+缩放+旋转 (8ch)
- 反照率+粗糙度 (4ch)
- 法线+可见性SH (19ch)

### 3. PCA姿态投影

```python
# 训练：对位置图执行PCA
β = S^T · (x - x̄)
# 测试：裁剪并重建
β_clipped = clip(β, -2σ, 2σ)
x_recon = S · β_clipped + x̄
```

### 4. 基于物理的渲染

**渲染方程**：
```math
L_o(x, \omega_o) = \int L_i \cdot f(x, \omega_i, \omega_o; \alpha, \gamma, n) \cdot (\omega_i \cdot n) d\omega_i
```

**本征分解**：
- 法线图 N(Θ)
- 反照率图 A(Θ)
- 粗糙度图 γ(Θ)
- 可见性 V(x, ωi)

---

## 三、网络架构

### DualStyleUNet

**输入**: 位置图 (512×512, 3ch)
**输出**: 双解码器输出 (1024×1024, 各属性通道)

**组件**：
- 编码器：多尺度下采样 + 条件卷积
- 双解码器：正面/背面独立预测
- 样式调制：ModulatedConv2d
- 噪声注入：可控随机性

**五个独立StyleUNet**：
1. Color Net (3ch)
2. Position Net (3ch)
3. Other Net (8ch)
4. Albedo & Roughness Net (4ch)
5. Normal & Visibility Net (19ch)

---

## 四、输入输出

### 训练输入

1. **多视图RGB视频**
   - THuman4.0: 24视角
   - AvatarReX: 16视角, 1500×2048
   - ActorsHQ: 47视角
   - 1500-3000帧

2. **SMPL-X参数**
   - body_pose, global_orient, transl
   - betas, expression
   - left/right_hand_pose, jaw_pose

3. **预处理数据**
   - 规范SMPL位置/法线图
   - LBS权重
   - 每帧姿态位置图

### 输出

1. **虚拟化身模型**
   - StyleUNet网络权重
   - 环境光图
   - 参数化模板

2. **渲染结果**
   - 动画图像序列
   - 本征分解(反照率/粗糙度/法线/可见性)
   - 重光照图像

---

## 五、训练与微调

### 训练配置

```yaml
优化器: Adam
学习率: 5e-4 (余弦退火)
批量大小: 1
总迭代数: 500k
训练时间: ~2天 (RTX 4090)
```

### 损失函数

| 损失项 | 权重 | 说明 |
|--------|------|------|
| L1 Loss | 1.0 | 重建损失 |
| LPIPS | 0.1 | 感知损失 |
| Normal | 0.2 | 法线监督 |
| Visibility | 0.1 | 可见性监督 |
| Geometry Reg | 0.01 | 几何正则化 |
| Material Smooth | 0.005 | 材质平滑 |

---

## 六、实验结果

### 定量对比

#### vs. NeRF-based Body-only Avatars

| 方法 | PSNR↑ | SSIM↑ | LPIPS↓ | FID↓ |
|------|-------|-------|--------|------|
| **Ours** | **28.0714** | **0.9739** | **0.0515** | **29.4831** |
| PoseVocab | 26.3784 | 0.9707 | 0.0592 | 49.4541 |
| SLRF | 26.9015 | 0.9724 | 0.0600 | 52.0613 |

#### vs. 3DGS-based Avatars

| 方法 | PSNR↑ | SSIM↑ | LPIPS↓ | FID↓ |
|------|-------|-------|--------|------|
| **Ours** | **30.3607** | **0.9682** | **0.0339** | **33.4665** |
| 3DGS-Avatar | 28.7836 | 0.9511 | 0.0418 | 49.3673 |
| GaussianAvatar | 26.9497 | 0.9389 | 0.0407 | 38.5387 |

### 性能指标

- **渲染速度** (动画): ~0.13秒/帧
- **渲染速度** (重光照): 4-10秒/帧
- **训练时间**: ~2天 (500k iterations, RTX 4090)

---

## 七、技术创新点

### 1. 显式3D高斯 + 2D CNN

**创新**：首次将显式3D Gaussian Splatting与强大的2D CNN结合
- 突破MLP低频偏差限制
- 实现高保真姿态依赖动态建模

### 2. 模板引导的参数化

**创新**：学习特定角色的可变形模板
- 适应穿着服装（包括长裙）
- 正交投影到正面/背面视图
- 保持3D高斯时间一致性

### 3. PCA姿态投影策略

**创新**：简单而有效的泛化方法
- 将新姿态投影到训练姿态分布内
- 系数裁剪约束在[-2σ, 2σ]
- 显著提升新姿态下的质量

### 4. PBR集成虚拟化身表示

**创新**：首次将PBR引入3DGS虚拟化身
- 分解材质(反照率/粗糙度/法线)和照明
- 支持新照明下的真实重光照
- 预测光照可见性SH系数

---

## 八、优势与局限

### 优势

✅ **高保真动态细节** - 显著优于NeRF-based方法  
✅ **实时渲染能力** - 动画约0.13秒/帧  
✅ **宽松衣物建模** - 成功建模长裙等宽松衣物  
✅ **新姿态泛化** - PCA投影策略提供合理插值  
✅ **重光照能力** - PBR分解材质和照明  
✅ **完整开源** - 代码、数据、预训练模型全部开源

### 局限性

❌ **身体和衣物纠缠** - 无法独立更换衣物  
❌ **依赖多视图输入** - 限制单目视频应用  
❌ **刚性组件建模不足** - 头发等非刚性组件物理运动有限  
❌ **重光照速度** - 4-10秒/帧

---

## 九、总结

Animatable Gaussians是CVPR 2024的重要工作，提出了一种**革新性的虚拟化身表示方法**：

### 核心贡献

1. **方法创新**: 首次将显式3D Gaussian Splatting与强大2D CNN结合
2. **表示优势**: 模板引导参数化 + 姿态依赖高斯图
3. **泛化策略**: 简单有效的PCA姿态投影
4. **功能扩展**: PBR实现可动画+可重光照

### 实验验证

- **定量**: 在3个数据集上显著优于SOTA方法
- **定性**: 高保真动态细节、真实重光照效果
- **性能**: 实时渲染速度(动画)

### 技术意义

Animatable Gaussians展示了**显式点云表示在虚拟化身建模中的巨大潜力**，为未来的高质量、实时、可交互虚拟化身研究指明了方向。

---

## 参考资源

- **论文**: https://arxiv.org/abs/2311.16096v4
- **代码**: https://github.com/lizhe00/AnimatableGaussians
- **项目**: https://animatable-gaussians.github.io/relight
- **数据集**: AvatarReX, THuman4.0, ActorsHQ

---

*最后更新: 2025年11月10日*
